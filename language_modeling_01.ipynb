{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i for i, s in enumerate(chars, 1)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E01:\n",
    "\n",
    "train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the trigram counting model\n",
    "\n",
    "N = torch.zeros(size=(27, 27, 27))\n",
    "\n",
    "for word in words:\n",
    "    chs = ['.'] + list(word) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
    "        N[ix1, ix2, ix3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aylah.\n",
      "on.\n",
      "el.\n",
      "ox.\n",
      "os.\n",
      "abia.\n",
      "ven.\n",
      "yah.\n",
      "ehann.\n",
      "emoe.\n"
     ]
    }
   ],
   "source": [
    "# sampling from the model\n",
    "\n",
    "# first, sample the second letter as a bigram model (because you only have the special dot token as input)\n",
    "\n",
    "\n",
    "P = N / N.sum(dim=2, keepdim=True)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix1 = 0\n",
    "    p = N[ix1].float()\n",
    "    p = p.sum(axis=0) / p.sum() # look for the probability of the second letter\n",
    "\n",
    "    ix2 = torch.multinomial(p, 1, replacement=True).item()\n",
    "\n",
    "    out.append(itos[ix2])\n",
    "\n",
    "# all letters from now on will depend on the previous two characters (trigram)\n",
    "    \n",
    "    while True:\n",
    "        p = P[ix1, ix2]\n",
    "        ix1 = ix2\n",
    "        ix2 = torch.multinomial(p, 1, replacement=True).item()\n",
    "        out.append(itos[ix2])\n",
    "        if ix2 == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0620)\n"
     ]
    }
   ],
   "source": [
    "# calculating the negative log likliehood\n",
    "\n",
    "nll = 0\n",
    "n = 0\n",
    "for word in words:\n",
    "    chs = ['.'] + list(word) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
    "        nll += -torch.log(P[ix1, ix2, ix3])\n",
    "        n += 1\n",
    "        \n",
    "print(nll/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the dataset\n",
    "\n",
    "xs, ys = [], []\n",
    "for word in words:\n",
    "    chs = ['.'] + list(word) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
    "        xs.append([ix1, ix2])\n",
    "        ys.append(ix3)\n",
    "        \n",
    "xs, ys = torch.tensor(xs), torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.248483657836914\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn(27*2, 27, requires_grad=True)\n",
    "\n",
    "for k in range(200):\n",
    "    xenc = F.one_hot(xs, num_classes=27).reshape(-1, 27*2).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "    loss = -probs[torch.arange(xs.shape[0]), ys].log().mean()\n",
    "#     print(loss.item())\n",
    "    \n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    W.data += -50*W.grad\n",
    "    \n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len.\n",
      "imkelderi.\n",
      "iar.\n",
      "ie.\n",
      "ah.\n",
      "azelen.\n",
      "ufzi.\n",
      "myn.\n",
      "wa.\n",
      "ola.\n"
     ]
    }
   ],
   "source": [
    "# sampling from the model\n",
    "\n",
    "# first, sample the second letter as a bigram model (because you only have the special dot token as input)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    \n",
    "    ix1 = None\n",
    "    ix2 = 0\n",
    "    \n",
    "    xenc = torch.concat([F.one_hot(torch.tensor([ix2]), num_classes=27), torch.zeros(1,27)], axis=1).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    p = counts / counts.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    ix1 = ix2\n",
    "    ix2 = torch.multinomial(p, 1, replacement=True).item()\n",
    "\n",
    "    out.append(itos[ix2])\n",
    "\n",
    "# all letters from now on will depend on the previous two characters (trigram)\n",
    "    \n",
    "    while True:\n",
    "        xenc = torch.concat([F.one_hot(torch.tensor([ix1]), num_classes=27),\n",
    "                             F.one_hot(torch.tensor([ix2]), num_classes=27)], axis=1).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        ix1 = ix2\n",
    "        ix2 = torch.multinomial(p, 1, replacement=True).item()\n",
    "        out.append(itos[ix2])\n",
    "        if ix2 == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E02: \n",
    "split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the dataset\n",
    "\n",
    "xs, ys = [], []\n",
    "for word in words:\n",
    "    chs = ['.'] + list(word) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
    "        xs.append([ix1, ix2])\n",
    "        ys.append(ix3)\n",
    "        \n",
    "xs, ys = torch.tensor(xs), torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156890 176501\n"
     ]
    }
   ],
   "source": [
    "train_split, dev_split = int(0.8*len(xs)), int(0.9*len(xs))\n",
    "print(train_split, dev_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling the dataset first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = list(range(xs.shape[0]))\n",
    "random.shuffle(idxs)\n",
    "\n",
    "xs, ys = xs[idxs], ys[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train, ys_train = xs[:train_split], ys[:train_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_dev, ys_dev = xs[train_split:dev_split], ys[train_split:dev_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_test, ys_test = xs[dev_split:], ys[dev_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*train set loss = 2.2469754219055176\n",
      "**dev set loss = 2.254650831222534\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn(27*2, 27, requires_grad=True)\n",
    "\n",
    "for k in range(200):\n",
    "    xenc = F.one_hot(xs_train, num_classes=27).reshape(-1, 27*2).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "    train_loss = -probs[torch.arange(xs_train.shape[0]), ys_train].log().mean()\n",
    "#     print(f'*train set loss = {loss.item()}')\n",
    "    \n",
    "    W.grad = None\n",
    "    train_loss.backward()\n",
    "    \n",
    "    W.data += -50*W.grad\n",
    "    \n",
    "    \n",
    "    ## eval on dev set\n",
    "    with torch.no_grad():\n",
    "        xenc = F.one_hot(xs_dev, num_classes=27).reshape(-1, 27*2).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "        dev_loss = -probs[torch.arange(xs_dev.shape[0]), ys_dev].log().mean()\n",
    "#         print(f'**dev set loss = {loss.item()}')\n",
    "    \n",
    "print(f'*train set loss = {train_loss.item()}')\n",
    "print(f'**dev set loss = {dev_loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**test set loss = 2.257509231567383\n"
     ]
    }
   ],
   "source": [
    "# eval on test set\n",
    "\n",
    "xenc = F.one_hot(xs_test, num_classes=27).reshape(-1, 27*2).float()\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "loss = -probs[torch.arange(xs_test.shape[0]), ys_test].log().mean()\n",
    "print(f'**test set loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E03: \n",
    "use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with alpha=0.0001\n",
      "**final dev set loss = 2.2562365531921387\n",
      "Testing with alpha=0.001\n",
      "**final dev set loss = 2.2491519451141357\n",
      "Testing with alpha=0.01\n",
      "**final dev set loss = 2.248021364212036\n",
      "Testing with alpha=0.1\n",
      "**final dev set loss = 2.2681424617767334\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn(27*2, 27, requires_grad=True)\n",
    "\n",
    "for alpha in [0.0001, 0.001, 0.01, 0.1]:\n",
    "    print(f'Testing with alpha={alpha}')\n",
    "    for k in range(200):\n",
    "        xenc = F.one_hot(xs_train, num_classes=27).reshape(-1, 27*2).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "        train_loss = -probs[torch.arange(xs_train.shape[0]), ys_train].log().mean() + alpha * (W**2).mean()\n",
    "#         print(f'*train set loss = {loss.item()}')\n",
    "\n",
    "        W.grad = None\n",
    "        train_loss.backward()\n",
    "\n",
    "        W.data += -50*W.grad\n",
    "\n",
    "\n",
    "        ## eval on dev set\n",
    "        with torch.no_grad():\n",
    "            xenc = F.one_hot(xs_dev, num_classes=27).reshape(-1, 27*2).float()\n",
    "            logits = xenc @ W\n",
    "            counts = logits.exp()\n",
    "            probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "            dev_loss = -probs[torch.arange(xs_dev.shape[0]), ys_dev].log().mean()\n",
    "    print(f'**final dev set loss = {dev_loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha of 0.01 seems to be the best choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**test set loss = 2.2707808017730713\n"
     ]
    }
   ],
   "source": [
    "xenc = F.one_hot(xs_test, num_classes=27).reshape(-1, 27*2).float()\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "loss = -probs[torch.arange(xs_test.shape[0]), ys_test].log().mean()\n",
    "print(f'**test set loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E04:\n",
    "we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*train set loss = 2.2586700916290283\n",
      "**dev set loss = 2.2561118602752686\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn(27*2, 27, requires_grad=True)\n",
    "\n",
    "xs_train_offset = xs_train.clone()\n",
    "xs_dev_offset = xs_dev.clone()\n",
    "\n",
    "xs_train_offset[:, 1] = xs_train[:, 1] + 27\n",
    "xs_dev_offset[:, 1] = xs_dev[:, 1] + 27\n",
    "\n",
    "for k in range(200):\n",
    "    logits = W[xs_train_offset].sum(axis=1)\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "    train_loss = -probs[torch.arange(xs_train.shape[0]), ys_train].log().mean() + 0.01 * (W**2).mean()\n",
    "#     print(f'*train set loss = {loss.item()}')\n",
    "\n",
    "    W.grad = None\n",
    "    train_loss.backward()\n",
    "\n",
    "    W.data += -50*W.grad\n",
    "\n",
    "\n",
    "    ## eval on dev set\n",
    "    with torch.no_grad():\n",
    "        logits = W[xs_dev_offset].sum(axis=1)\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "        dev_loss = -probs[torch.arange(xs_dev.shape[0]), ys_dev].log().mean()\n",
    "#         print(f'**dev set loss = {loss.item()}')\n",
    "\n",
    "\n",
    "print(f'*train set loss = {train_loss.item()}')\n",
    "print(f'**dev set loss = {dev_loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E05:\n",
    "look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*train set loss = 2.247150182723999\n",
      "**dev set loss = 2.2552051544189453\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn(27*2, 27, requires_grad=True)\n",
    "\n",
    "for k in range(200):\n",
    "    xenc = F.one_hot(xs_train, num_classes=27).reshape(-1, 27*2).float()\n",
    "    logits = xenc @ W\n",
    "#     counts = logits.exp()\n",
    "#     probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "    train_loss = F.cross_entropy(logits, ys_train)\n",
    "#     print(f'*train set loss = {loss.item()}')\n",
    "    \n",
    "    W.grad = None\n",
    "    train_loss.backward()\n",
    "    \n",
    "    W.data += -50*W.grad\n",
    "    \n",
    "    \n",
    "    ## eval on dev set\n",
    "    xenc = F.one_hot(xs_dev, num_classes=27).reshape(-1, 27*2).float()\n",
    "    logits = xenc @ W\n",
    "#     counts = logits.exp()\n",
    "#     probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "    dev_loss = F.cross_entropy(logits, ys_dev)\n",
    "#     print(f'**dev set loss = {loss.item()}')\n",
    "    \n",
    "print(f'*train set loss = {train_loss.item()}')\n",
    "print(f'**dev set loss = {dev_loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E06: \n",
    "meta-exercise! Think of a fun/interesting exercise and complete it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5141541957855225\n"
     ]
    }
   ],
   "source": [
    "# combining input words through addition instead of concat\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "W = torch.randn(27, 27, requires_grad=True)\n",
    "\n",
    "for k in range(200):\n",
    "    xenc = F.one_hot(xs, num_classes=27).sum(dim=1).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(axis=1, keepdims=True)\n",
    "    loss = -probs[torch.arange(xs.shape[0]), ys].log().mean()\n",
    "#     print(loss.item())\n",
    "    \n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    W.data += -50*W.grad\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aveys.\n",
      "etykaeleidegahnansaimalbi.\n",
      "aanienndgiamiahreyanvo.\n",
      "quetzekyniaslahnanemkariahnesdelnannaaih.\n",
      "anfarlieigente.\n",
      "vyilnasfiaadveobleevnaynkeppenantlasgexlbaryf.\n",
      "ydielnaderniealinecaenoinetniaslanianlioadne.\n",
      "aihalivlinalryonniaskaylaotelnarnanemriyncise.\n",
      "hal.\n",
      "halyadreadermeymric.\n"
     ]
    }
   ],
   "source": [
    "# sampling from the model\n",
    "\n",
    "# first, sample the second letter as a bigram model (because you only have the special dot token as input)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    \n",
    "    ix1 = None\n",
    "    ix2 = 0\n",
    "    \n",
    "    xenc = torch.concat([F.one_hot(torch.tensor([ix2]), num_classes=27)], axis=1).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    p = counts / counts.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    ix1 = ix2\n",
    "    ix2 = torch.multinomial(p, 1, replacement=True).item()\n",
    "\n",
    "    out.append(itos[ix2])\n",
    "\n",
    "# all letters from now on will depend on the previous two characters (trigram)\n",
    "    \n",
    "    while True:\n",
    "        xenc = (F.one_hot(torch.tensor([ix1]), num_classes=27) + F.one_hot(torch.tensor([ix2]), num_classes=27)).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        ix1 = ix2\n",
    "        ix2 = torch.multinomial(p, 1, replacement=True).item()\n",
    "        out.append(itos[ix2])\n",
    "        if ix2 == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
